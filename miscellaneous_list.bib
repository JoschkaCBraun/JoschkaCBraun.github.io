---
---
@inproceedings{Braun_Exploration_Hacking_Blog_Post_2025,
    author    = {Braun, Joschka and Falck, Damon and Jang, Yeonwoo},
    title     = {Exploration hacking: can reasoning models subvert RL?},
    year      = {2025},
    html      = {https://www.alignmentforum.org/posts/Dft9vpMnEeWFE3Gc6/exploration-hacking-can-reasoning-models-subvert-rl-1},
    img       = {assets/img/misc/exploration_hacking_af_post.jpeg},
    booktitle = {Exploration hacking, where models subvert their RL training by selectively under-exploring, could threaten both the development of beneficial capabilities and the effectiveness of safety training.},
}
@inproceedings{Braun_Anthropic_Hackathon_2025,
    author    = {Braun, Joschka and Falck, Damon and Jang, Yeonwoo},
    title     = {Anthropic Alignment Hackathon: Exploration Hacking},
    year      = {2025},
    slides    = {assets/pdf/anthropic_hackathon_2025.pdf},
    img       = {assets/img/misc/anthropic_hackathon_2025.jpeg},
    booktitle = {Participated in the June 2025 <a href="https://www.anthropic.com/" target="_blank">Anthropic</a> Alignment Hackathon in San Francisco, co-developing an "Exploration Hacking" prototype over two days alongside Damon Falck and Yeonwoo Jang.},
}
@misc{rein2025hcasthumancalibratedautonomysoftware,
  author       = {David Rein et al.},
  title        = {HCAST: Human-Calibrated Autonomy Software Tasks},
  year         = {2025},
  html         = {https://arxiv.org/abs/2503.17354},
  arxiv         = {https://arxiv.org/abs/2503.17354}, 
  eprint       = {2503.17354},
  archivePrefix= {arXiv},
  primaryClass = {cs.AI},
  booktitle    = {Acknowledged Contributor to HCAST benchmark: Contributed task feedback and set human performance baselines for ML Engineering tasks.}, 
  img          = {assets/img/misc/hcast_paper.jpeg}, 
}

