---
---
@inproceedings{Braun_Exploration_Hacking_Blog_Post_2026,
    author    = {Braun, Joschka and Falck, Damon and Jang, Yeonwoo},
    title     = {A Conceptual Framework for Exploration Hacking},
    year      = {2026},
    html      = {https://www.alignmentforum.org/posts/suRWiTNnazrRsoBKR/a-conceptual-framework-for-exploration-hacking},
    img       = {assets/img/misc/eh_conceptual_af_post_2026.jpeg},
    booktitle = {We formalize and decompose exploration hacking, where models strategically alter their exploration to resist RL training, presenting a conceptual framework and open problems ahead of our upcoming empirical paper.},
}

@inproceedings{Braun_Exploration_Hacking_Blog_Post_2025,
    author    = {Falck, Damon and Braun, Joschka and Jang, Yeonwoo},
    title     = {Exploration hacking: can reasoning models subvert RL?},
    year      = {2025},
    html      = {https://www.alignmentforum.org/posts/Dft9vpMnEeWFE3Gc6/exploration-hacking-can-reasoning-models-subvert-rl-1},
    img       = {assets/img/misc/exploration_hacking_af_post.jpeg},
    booktitle = {Exploration hacking, where models subvert their RL training by selectively under-exploring, could threaten both the development of beneficial capabilities and the effectiveness of safety training.},
}
@inproceedings{Braun_Anthropic_Hackathon_2025,
    author    = {Braun, Joschka and Falck, Damon and Jang, Yeonwoo},
    title     = {Anthropic Alignment Hackathon: Exploration Hacking},
    year      = {2025},
    slides    = {assets/pdf/anthropic_hackathon_2025.pdf},
    img       = {assets/img/misc/anthropic_hackathon_2025.jpeg},
    booktitle = {Participated in the June 2025 <a href="https://www.anthropic.com/" target="_blank">Anthropic</a> Alignment Hackathon in San Francisco, co-developing an "Exploration Hacking" prototype over two days alongside Damon Falck and Yeonwoo Jang.},
}
@misc{rein2025hcasthumancalibratedautonomysoftware,
  author       = {David Rein et al.},
  title        = {HCAST: Human-Calibrated Autonomy Software Tasks},
  year         = {2025},
  html         = {https://arxiv.org/abs/2503.17354},
  arxiv         = {https://arxiv.org/abs/2503.17354}, 
  eprint       = {2503.17354},
  archivePrefix= {arXiv},
  primaryClass = {cs.AI},
  booktitle    = {Acknowledged Contributor to HCAST benchmark: Contributed task feedback and set human performance baselines for ML Engineering tasks.}, 
  img          = {assets/img/misc/hcast_paper.jpeg}, 
}

@inproceedings{Braun_Steering_Blog_Post_2024,
    author    = {Braun, Joschka and Krasheninnikov, Dmitrii and Anwar, Usman and Kirk, Robert and Tan, Daniel and Krueger, David},
    title     = {A Sober Look at Steering Vectors for LLMs},
    year      = {2024},
    html      = {https://www.alignmentforum.org/posts/QQP4nq7TXg89CJGBh/a-sober-look-at-steering-vectors-for-llms},
    img       = {assets/img/misc/steering_vector_blog_post.jpeg},
    pdf       = {assets/pdf/steering_blog_post.pdf},
    booktitle = {Blog post on the key challenges in controlling LLM behaviour with steering vectors. Published on The Alignment Forum.},
}